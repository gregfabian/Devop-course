- WHY DEVOPS IS IMPORTANT?
  - SOFTWARE COMPANY:
    - Developers
      - The role of a developer is to create applications and hand over their code to the operations team.
      - Previously, the developer would execute the code on their own system and then send it to the operations 	team.
    - Operations
      - The operations team is responsible for testing the code and giving feedback to developers if any bugs are 	found. If everything is satisfactory, the operations team uploads the code to the build servers.
      - However, when the operations team attempted to run the code on their system, it failed to execute!

  - Despite improvements in software quality, there remained inefficiencies within the development team. A typical                        	software development team includes both Developers and Operations personnel.
  - This leads to a lot of back and forth between the developer and the operations team, hence impacted    	efficiency.
  - So this can be solved sing Devops!

  -	TRADITIONAL IT			VS	DEVOPS
  -------------------------------------------------------------------
	Less Productive				More Productive
	Skill Centric Teams			Teams is divided into specialized silos(units/technology)
	More Time invested in planning		Smaller and Frequent releases lead to easy scheduling and less 							time in planning
	Difficult to achieve target or goal	Frequent releases, with continuous feedback makes achieving 							targets easy
 --------------------------------------------------------------------
 1. Predictability: DevOps offers a significantly lower failure rate of new releases.

 2. Reproducibility: Version everything so that earlier versions can be restored anytime.

 3. Maintainability: Effortless recovery process in the event of a new release crashing or disabling the current   system.

 4. Time to market: DevOps reduces the time to market up to 50% through streamlined software delivery. It is particularly the case for digital and mobile applications.

 5. Greater Quality: DevOps helps the team improve application development quality by incorporating infrastructure issues.

 6. Reduced Risk: DevOps incorporates security aspects in the software delivery lifecycle, and it helps reduce defects across the lifecycle.

 7. Resiliency: The Operational state of the software system is more stable, secure, and changes are auditable.

 8. Cost Efficiency: DevOps offers cost efficiency in the software development process, which is always an aspiration of IT management.

 9. Breaks larger code base into small pieces: DevOps is based on the agile programming method. Therefore, it allows breaking larger codebases into smaller and manageable chunks.


- WHAT IS DEVOPS?
	- Devops is a software development methodology/idea which improves the collaboration between developers and operations teams using various automation tools. These automation tools are implemented using various stages which are a part of the Devops Lifecycle.

	1. Customer-Centric Action: The DevOps team must constantly take customer-centric action to invest in products and services.

	2. End-To-End Responsibility: The DevOps team needs to provide performance support until they become end-of-life. This enhances the level of responsibility and the quality of the products engineered.

	3. Continuous Improvement: DevOps culture focuses on continuous improvement to minimize waste, and it continuously speeds up the improvement of products or services offered.

	4. Automate everything: Automation is a vital principle of the DevOps process, and this is not only for software development but also for the entire infrastructure landscape.

	5. Work as one team: In the DevOps culture, the designer, developer, and tester are already defined, and all they need to do is work as one team with complete collaboration.

	6. Monitor and test everything: Monitor and test everything: The DevOps team needs robust monitoring and testing procedures.



- HOW TO LEARN DEVOPS?



- HOW DEVOPS WORKS?
	- The DevOps Lifecycle is a series of development stages that guide everyone as efficiently as possible through the end-to-end process of product development. All of these components of the DevOps lifecycle is necessary to take the maximum leverage of the DevOps methodology.
	Stage 1) Continuous Development
		This practice spans the planning and coding phases of the DevOps lifecycle. Version-control mechanisms might be involved.

	Stage 2) Continuous Integration
		This software engineering practice develops software by frequently integrating its components. It helps to ensure that changes in the source code do not break the build or cause other problems. This is the connection between the Developers and Operations(DEV <- integration -> OPs).

	Stage 3) Continuous Testing
		This DevOps lifecycle stage incorporates automated, prescheduled, continued code tests as application code is written or updated. Such tests can be written manually or in conjunction with continuous integration tools.

	Stage 4) Continuous Deployment
		The deployment process takes place continuously in this DevOps lifecycle phase. It is performed so that any changes made in the code should not affect the functioning of a high traffic website or app. It could be deployed on a test server or production server.

	Stage 5) Continuous Monitoring
		During this phase, developers collect data, monitor each function, and spot errors like low memory or server connection are broken. For example, when users log in, they should access their account, and a failure to do so means there’s a problem with your application.

	Stage 6) Continuous Feedback
		Continuous feedback is like a progress report or log. In this DevOps stage, the software automatically sends out information about performance and issues experienced by the end-user. It’s also an opportunity for customers to share their experiences and provide feedback.

	Stage 7) Continuous Operations
		It is the last, shortest, and most straightforward phase of DevOps. It also involves automating the application’s release and all these updates that help you keep cycles short and give developers and provide more time to focus on developing.



WHAT EAXCTLY HAPPENS AT EACH STAGE:
	- Continuous Development:
		This stage involves committing codes to version control tools such as Git or SVN for maintaining the different versions of the code, and tools like Ant, Maven, Gradle for building/packaging the code into an executable file that can be forwarded to the QAs for testing.
	- Continuous Integration:
		The stage is a critical point in the whole Devops Lifecycle. It deals with integrating the different stages of the Devops lifecycle, and is therefore the key in automating the whole Devops Process.
	- Continuous Deployment:
		In this stage the code is built, the environment or the application is containerized and is pushed on to the desired server. The key processes in this stage are Configuration Management, Virtualization and Containerization.
	- Continuous Testing:
		The stage deals with automated testing of the application pushed by the developer. If there is an error, the message is sent back to the integration tool, this tool in turn notifies the developer of the error. If the test was a success, the message is sent to Integration tool which pushes the build on the production server.
	- Continuous Monitoring:
		The stage continuously monitors the deployed application for bugs or crashes. It can also be setup to collect user feedback. The collected data is then sent to the developers to improve the application.

-> Benefits of the DevOps lifecycle
	Here are some essential benefits of the DevOps lifecycle:
	- The DevOps lifecycle is a useful approach that guides developers and IT operations professionals through the complex process of app creation.
	- Better efficiencies indeed lead to a higher return on investment.
	- Widely used by large and small teams can use it to help them organize, align, and track phases in the life cycle.
	- Automatic monitoring, testing, and releases help developers detect and fix bugs early.
	- Multiple automated methods for collecting feedback mean DevOps developers can learn more about their products and improve the quality of their code.



- IMPORTANT DEVOPS TOOLS:
We have discussed the Devops Methodology, but this methodology cannot be put into action without it's corresponding tools. Let us discuss the Devops tools with their respective lifecycle stages.
  >- git:
	>- Why git?
		- Git is the most popular tool among all the distributed version-control system (DVCS) tool

	>- What is Git?
		- Git is a version-control system for tracking changes in computer files and coordinating work on those files among multiple people. It is primarily used for source-code management in software development, but it can be used to keep track of changes in any set of files.

	>- Common Git Commands:
		- You can do the following tasks, when working with git. Let us explore the commands related to each of these tasks:
			- Creating Repository:
				- You can create a repository using the command 'git init'. Navigate to your project folder and enter the command git init to initialize a git repository for your project on the local system.
 
			- Making Changes:
				- Once the directory has been initialized you can check the status of the files, whether they are being tracked by git or not, using the command 'git status'.
				- Since no files are being tracked right now, let us now stage these files. For that, enter the command 'git add .'. If we want to track all the files in the project folder, we can type the command 'git add'.
				- Once files or changes have been staged, we are ready to commit them in our repository. We can commit the files using the command 'git commit -m "custom message"'.

			- Syncing Repositories:
				- Once everything is ready on our local, we can start pushing our changes to the remote repository. Copy your repository link and paste it in the command 'git remote add origin "<URL to repository>"'.
				- To push the changes to your repository, enter the command 'git push origin <branch-name>' and hit enter. In our case the branch is master, hence 'git push origin master'. This command will then prompt for username and password, enter the values and hit enter.
				- Similarly, if we want to download the remote repository to our local system, we can use the command 'git clone <URL>'. This command will create a folder with the repository name, and download all the contents of the repository inside this folder. In our example, repository contents were downloaded into the "Devops" folder.
				- The git pull command is also used for pulling the latest changes from the repository, unlike git clone, this command can only work inside an initialized git repository. This command is used when you are already working in the cloned repository, and want to pull the latest changes, that others might have pushed to the remote repository. 'git pull <URL of link>'
			- Parallel Development
				- Until now, we saw how you can work on git. But now imagine, multiple developers working on the same project or repository. To handle the workspace of multiple developers, we use branches. To create a branch from an existing branch, we type 'git branch <name-of-new-branch>'. Similarly, to delete a branch use the command 'git branch -D <branch name>'.
				- To switch to the new branch, we type the command 'git checkout <branch-name>'
				- Want to check the log for every commit detail in your repository? You can accomplish that using the command 'git log'.
				- Want to save your work without committing the code? Git has got you covered. This can be helpful when you want to switch branches, but do not want to save your work to your git repository. To stash your staged files without committing just type in 'git stash'. If you want to stash your untracked files as well, type 'git stash -u'. Once you are back and want to retrieve working, type in 'git stash pop'.
				- This command helps you in reverting a commit, to a previous version 'git revert <commit-id>'. <commit-id> can be obtained from the output of 'git log'.
				- This command helps us in checking the differences between two versions of a file 'git diff <commit-id of version x> <commit-id of version y>'. <commit-id> can be obtained from the output of 'git log'
			
   
  -> Docker:
---------------
	- Problems Before Docker:
	-------------------------
		- Imagine you are a Developer, and you are creating a website on PHP, you need OS, PHP software, Libraries for you to be able to do that. Developers used to run the code on their system, it would run perfectly. But the same code did not run on the operations team system.
	- The Problem:
	----------------
		- Developer		<-PHP Code->		Operations/Testing
		- Works fine on my system!			- Doesn't work on my system. Faulty code!
		- We needed an entity which can "contain" all the software dependencies, and can be ported on to other computers as plug & play package.
	- The Answer:
		- Developer gives the environment to Operation team for further testing and deploying on production.
	- What is Docker?
	-----------------
		- Docker is a computer program that performs operating-system-level virtualization, also known as "containerization". It was first released in 2013 and is developed by Docker, Inc. Docker is used to run software packages called "containers".
		- Docker is a software development platform for virtualization with multiple Operating systems running on the same host. It helps to separate infrastructure and applications in order to deliver software quickly. Unlike 'Hypervisors', which are used for creating VM (Virtual machines), virtualization in Docker is performed on system-level, also called Docker containers. Docker containers run on top of the host’s Operation system. This helps you to improves efficiency and security. Moreover, we can run more containers on the same infrastructure than we can run Virtual machines because containers use fewer resources.
		- A hypervisor is a type of software, hardware, or firmware that allows a computer to run multiple virtual machines, or guest machines, on a single physical machine, or host machine. The hypervisor allocates the host machine's resources, like memory and CPU, to each virtual machine as needed.
		- Application containerization is an OS-level Virtualization method used to deploy and run distributed applications without launching an entire Virtual Machin (VM) for each app.
		-> Virtualization in Docker vs Hypervisor:
		------------------------------------------
			- Unlike the VMs which can communicate with the hardware of the host (ex: Ethernet adapter to create more virtual adapters) Docker containers run in an isolated environment on top of the host’s OS. Even if your host runs Windows OS, you can have Linux images running in containers with the help of Hyper-V, which automatically creates small VM to virtualize the system’s base image, in this case, Linux.
		
	-> Why use Docker?
	-------------------
		- Docker is computer software used for Virtualization in order to have multiple Operating systems running on the same host
		- Docker is the client-server type of application which means we have clients who relay to the server
		- Docker images are the “source code” for our containers; we use them to build
		- Docker file has two types of registries 1.) public and 2.)private registries
		- Containers are the organizational units of Docker volume. In simple terms, an image is a template, and a container is a copy of that template. You can have multiple containers (copies) of the same image.
	-> Docker Architecture:
	------------------------
			-> Docker Engine:
				- Docker is the client-server type of application which means we have clients who relay to the server. So the Docker daemon called: Dockerd is the Docker engine which represents the server. The docker daemon and the clients can be run on the same or remote host, and they communicate through command line client binary, as well as a full RESTful API to interact with the daemon: Dockerd.
			-> Docker Images:
				- Docker images are the “source code” for our containers; we use them to build containers. They can have software pre-installed which speeds up deployment. They are portable, and we can use existing images or build our own.
			-> Docker Registries:
				- Docker stores the images we build in registries. There are public and private registries. Docker company has public registry called 'Docker hub', where you can also store images privately. Docker hub has millions of images, which you can start using now.
	-> Docker Containers:
	---------------------
		- Containers are the organizational units and one of the Docker basics concept. When we build an image and start running it; we are running in a container. The container analogy is used because of the portability of the software we have running in our container. We can move it, in other words, “ship” the software, modify, manage, create or get rid of it, destroy it, just as cargo ships can do with real containers.

In simple terms, an image is a template, and a container is a copy of that template. You can have multiple containers (copies) of the same image.
	-> Common Docker Operations:
	----------------------------
		- Version: e.g. 'docker --version'. This command helps you know the installed version of the docker software on your system.
		- Pull: e.g. 'docker pull <image-name>'. This command helps you pull images from the central docker repository.
		- images: e.g. 'docker images'. This command helps you in listing all the docker images, downloaded on your system. 
			-What is a Docker Image?	
			------------------------
				- A Docker image is a lightweight, standalone, and executable package that includes everything needed to run a piece of software, including the code, runtime, libraries, environment variables, and configuration files. Docker images are used to create Docker containers, which are instances of the image running in an isolated environment. Think of a Docker image as a blueprint or template for building a container.
		- Run: e.g. 'docker run -it -d <image-name>'. This command helps in running containers, from their image name. '-it' means make the container interactive, '-d' make the container a daemon that it should be running in the background even though I'm not working on it.
		- Running containers: e.g. 'docker ps'. This command helps in listing all the containers which are running in the system.
		- View Running Container states: e.g. 'docker ps -a'. If there are any stopped containers, they can be seen by adding the "-a" flag in this command.
		- Exec or enter into the container: e.g. 'docker exec -it <container-id> bash'. For logging into/accessing the container, one can use the exec command. '-it' means make it interactive, 'bash' means I want to work on the current terminal.
		- Stop the container: e.g. 'docker stop <container-id>'. For stopping a running container, we use the "stop" command.
		- Kill the container: e.g. 'docker kill <container-id>'. This command kills the container by stopping its execution immediately. The difference between 'docker kill' and 'docker stop'. 'docker stop' gives the container time to shutdown gracefully, in situations when it is taking too much time for getting the container to stop, one can opt to kill it. 
		- Remove a container: e.g. 'docker rm <container-id>'. To remove a stopped container from the system, we use the "rm" command.
		- Remove an Image: e.g. 'docker rmi <image-id>'. To remove an image from the system we use the command "rmi". 
		- Saving Changes to a container: e.g. 'docker commit <container-id> <name-of-image>'. With this command, a new image is created which can be seen under 'docker images' with the same name as passed in the command.
		-> 1. Stop All Running Containers:
		-----------------------------------
			Use the following command to stop all running containers:
				-> for /F "tokens=*" %i IN ('docker ps -q') DO docker stop %i
		-> 2. Remove All Containers:
		-----------------------------------
			Use this command to remove all containers (both running and stopped):
				-> for /F "tokens=*" %i IN ('docker ps -aq') DO docker rm -f %i
				Explanation
					for /F "tokens=* %i IN ('docker ps -q') DO ...:
					Runs the docker ps -q command to get container IDs.
					Iterates through the IDs (%i).
					Executes docker stop or docker rm -f for each container ID.

		- Mapping Ports on containers: e.g. 'docker run -it -p 82:80 -d userId/apache'. For instances after installing apache in the container the apache runs on port 80, but the host operating system port runs on port 82. So the check if the apache works or not we have to map the ports against each other. -p port number you want to specify: Port number of the container that you want to link it with.
			-> Port Mapping in Docker:
			--------------------------
				- Port mapping in Docker is a way to expose a container's internal network ports to the host machine or the outside world. Containers run in an isolated environment, and by default, they are not accessible externally. Port mapping allows you to bind a container's internal port to a port on the host machine.
				- How Port Mapping Works
					- When you use the -p flag in Docker, you specify two ports:
					    e.g: 'docker run -d -p <host_port>:<container_port> <image_name>'
						1. Host Port (<host_port>):
							- The port on your host machine that external clients will connect to.
						2. Container Port (<container_port>):
							The port inside the container where the service (e.g., Apache) is running.
					- When someone accesses http://<host_ip>:82, the Docker engine forwards that request to port 80 inside the container.
		- Pushing a container to docker hub:
	-> Introduction to Dockerfile:
	------------------------------
		- A Dockerfile is a text document that contains all the commands a user could call on the command line to assemble an image. Using Docker build users can create an automated build that executes several command-line instructions in succession.
			-> Various Commands in Dockerfile:
				- The FROM keyword is used to define the base image, on which we will be building. e.g. FROM ubuntu.
				- The ADD keyword is used to add files to the container being build. The syntax which is followed is ADD<source> <destination in container>. 
					e.g. FROM ubuntu
					     ADD ./var/www/html
				- The RUN keyword is used to add layers to the base image, by installing components. Each RUN statement, adds a new layer to the docker image.
					e.g. FROM ubuntu
					     RUN apt-get update
					     RUN apt-get -y install apache2
					     ADD ./var/www/html
				- The CMD keyword is used to run commands on the start of the container. These commands run only when there is no argument specified while running the container.
					e.g. FROM ubuntu
					     RUN apt-get update
					     RUN apt-get -y install apache2
					     ADD ./var/www/html
					     CMD apachectl -D FOREGROUN
				- The ENTRYPOINT keyword is used strictly run commands the moment the container initializes. The difference between CMD and ENTRYPOINT is, ENTRYPOINT will run irrespective of the fact whether argument is specified or not.
					e.g. FROM ubuntu
					     RUN apt-get update
					     RUN apt-get -y install apache2
					     ADD ./var/www/html
					     CMD apachectl -D FOREGROUND
					     ENTRYPOINT apachectl -D FOREGROUND
				- The ENV keyword is used to define environment variables in the container run-time.
					e.g. FROM ubuntu
					     RUN apt-get update
					     RUN apt-get -y install apache2
					     ADD ./var/www/html
					     CMD apachectl -D FOREGROUND
					     ENTRYPOINT apachectl -D FOREGROUND -- with this we didn't need to start the apache services ourselves.
					     ENV name Devops Fortesoft
	-> Docker Volumes:
	------------------
		- Introduction to Docker Volumes:
			- Docker Volumes are used to persist data across the lifetime of a container. This basically map or host the container outside the container so when you delete the container the storage still remains outside. And what ever you do is saved inside the host volumes outside the container rather than the container.
			- Bind mount vs Docker Volumes:
				- Bind mount is when you mount a particular file location inside the container, while volumes are managed by docker unlike the bind mount which is decided by which directory you use.
		- Creating a Docker Volume:
			e.g. 'docker volume create <name-of-volume>'.
		- View List Of Volumes: 
			e.g. 'docker volume ls'.
		- Attaching the volume to the container:
			e.g. 'docker run -it --mount source=<name-of-volume>, target=<path-to-directory> -d <image-name>'. NOTE: When you create a volume you can access it through any container. 2:50:59
	       NOTE: The volume is not attached to the image, it can only be accessed when you mount. 2:55:07
	       NOTE: Docker volumes are not attached to images. Volumes are independent from both images and containers.
	
		
	-> Microservices: Sometimes we have so many containers running simultaneously, or multiple containers working with each other. To understand this microservices we need to understand monolithic application.
		-> What is a Monolithic Application:	
			- A Monolithic application is a single-tiered software application in which different components are combined into a single program which resides in a single platform. 

			- Single-tiered software, also known as one-tier architecture, is a software application architecture that puts all required components on a single server or machine: 

			- User interface
			The user interface is accessible by the application on the same local drive. 

			- Business logic
			The backend business logic is located on the same machine as the user interface and database. 

			- Database
			The database is located on the same machine as the user interface and backend business logic.	e.g. Uber App e.t.c.
		-> Disadvantages Of a Monolithic Application:
			❌ Application is large and complex to understand.
			❌ Entire Application has to re-deploy on an application update.
			❌ Bug in any module, can bring down entire application. This is because in a monolithic application everything is dependent on each other. 
			❌ Has a barrier to adopting new technologies. That means all your module will stick to a single language or technology example java, all your module and features will have to be in one particular language.
		-> What are Microservices?
			- Microservices are a software development architectural style that structures an application as a collection of loosely coupled services. They're not dependent on each other unlike the Monolithic applications. e.g. using API to serve other features in an application.
		-> Advantages of Microservices:
			✅ Application is distributed, hence easy to understand.
			✅ The code of only the microservice which is supposed to be updated is changed.
			✅ No barrier to any specific technology.

	-> What is Docker Compose?
		- Compose is a tool for defining and running multi-container Docker applications. With Compose, you use a YAML file to configure your application's services. Then, with a single command, you create and start all the services from your configuration. Run 'docker-compose' up and compose starts and runs your entire app.
		- Command for docker compose:
			e.g. 'docker-compose up -d'
			NOTE: When you remove the container for the compose the server will stop and the browser won't display anything. This leads us to using container orchestration.

	-> Container Orchestration Tool: 
		- This is when you have multiple containers launched and you want to monitor their health. This is a system that automates the deployment, management, scaling, networking, and monitoring of containerized applications. While Docker itself allows you to build and run containers, orchestration tools take it a step further by managing containers in complex environments, such as clusters, with multiple nodes and workloads. 
		
		- What is docker Swarm:
			- Docker Swarm is a clustering and scheduling tool for Docker containers. With Swarm, IT administrators and Developers can establish and manage a cluster of Docker nodes as a single virtual system. Docker Swarm is a native clustering and orchestration tool for Docker containers. It allows you to manage and orchestrate a group of Docker engines (nodes) as a single virtual system. With Docker Swarm, you can deploy, scale, and manage containerized applications efficiently, making it easier to handle multiple containers across multiple servers. With Docker Swarm you will get the functionality of automatically monitoring the health of the containers and it helps you keep a healthy number of containers that you have specified always in the running state. A Docker Swarm cannot work with just one machine, because light containers or even machines can be faulty sometimes, you may configure docker swarm on a single machine but can automatically repair a container, so what happens if the machine goes down, so to mitigate those kind of problems, we came up with distributed kind of architecture where in you will have multiple machines running in the swarm so there will be one machine that will be called the 'Leader' which will tell the 'workers' what will do and the workers will have the containers running on them, while they (workers) run on the cluster and also the workers will run the containers we wanna launch.

			- To activate swarm:
				e.g. "docker swarm init". After running this, Docker will initialize the current node as the Swarm Manager and provide a command to join other nodes.
			- Verify Swarm Status:
				e.g. "docker info". The output should show: Swarm: active
			- Confirm Manager IP Address: If the Manager IP isn't clear, you can manually inspect it by using: 
			e.g. "docker node ls"
18.224.7.58:82
  - Puppet
  - Ansible
  - Jenkins
  - Kubernetes
  - Nagios
- Devops Tools Based on stages:
-------------------------------
	- Continuous Development:
		- Git is a distributed version-control system for tracking changes in computer files and coordinating work on those files among multiple people. It is primarily used for source-code management in software development, but it can be used to keep track of changes in any set of files.
	- Continuous Integration:
		- Jenkins is an open source automation server written in Java. Jenkins helps to automate the non-human part of the software development process, with continuous integration and facilitating technical aspects of continuous delivery.
	- Continuous Deployment:
		- Virtualization and Containerization -> Docker. Configuration Management -> Puppet
	- Continuous Testing:
		- Selenium is a portable software-testing framework used for web applications. It is an open source tool which is used for automating the tests carried out on web browsers (Web applications are tested using any web browser).
	- Continuous Monitoring:
		- Nagios is an open-source Devops tool which is used for monitoring systems, networks and infrastructure. It also offers monitoring and alerting services for any configurable event.

- IMPORTANT INTERVIEW QUESTIONS
QUIZ 1:
	- What is the primary objective of adopting Devops?
		1. Faster software delivery ✅
		2. Reducing team size
		3. Increasing manual tasks
		4. Avoiding cloud technology

QUIZ 2:
	- Which of the following best describes a key difference between Traditional IT and DevOps?
		1. DevOps emphasizes collaboration ✅
		2. Traditional IT focuses on automation
		3. DevOps eliminates testing 
		4. Traditional IT avoids using cloud services

3:20:43
--Docker Installation Windows
-- https://store.docker.com/editions/community/docker-ce-desktop-windows



https://www.youtube.com/watch?v=PFQBBYA7D_k












